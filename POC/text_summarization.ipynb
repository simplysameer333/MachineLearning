{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import listdir\n",
    "import time\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import pickle\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config & Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'G:\\\\AI\\\\data\\\\cnn\\\\'\n",
    "path = base_path + 'sample_5k\\\\'\n",
    "articles_pickle_filename = \"articles.pickle\"\n",
    "headlines_pickle_filename = \"headlines.pickle\"\n",
    "model_pickle_filename = \"model.pickle\"\n",
    "word_embedding_matrix_filename = \"word_embedding_matrix.pickle\"\n",
    "\n",
    "''' https://fasttext.cc/docs/en/english-vectors.html '''\n",
    "model_path ='G:\\Python\\MLLearning\\MachineLearning\\data\\wiki-news-300d-1M.vec'\n",
    "\n",
    "# to avoid words that are used less that threshold value\n",
    "threshold = 2\n",
    "\n",
    "# Dimension size as per pre-trained data\n",
    "embedding_dim = 300\n",
    "max_text_length = 1000\n",
    "max_summary_length = 20\n",
    "min_length = 2\n",
    "unk_text_limit = 200\n",
    "\n",
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword list and Initialize Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files and load into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split a document into news article body and headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(doc):\n",
    "    # find first headlines\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and headlines\n",
    "    article, headlines = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    headlines = [h.strip() for h in headlines if len(h) > 0]\n",
    "    return article, headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean a list of lines\n",
    "This section is used to remove unwanted words and return cleaned articles and headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lines, remove_stopwords=True):\n",
    "    \n",
    "    cleaned = list()\n",
    "    for line in lines:\n",
    "        # strip source cnn office if it exists\n",
    "        index = line.find('(CNN)  -- ')\n",
    "        if index > -1:\n",
    "            line = line[index + len('(CNN)'):]\n",
    "        else:\n",
    "            index = line.find('(CNN)')\n",
    "            if index > -1:\n",
    "                line = line[index + len('(CNN)'):]\n",
    "\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "\n",
    "        # Optionally, remove stop words\n",
    "        if remove_stopwords:\n",
    "            line = [w for w in line if w not in stop_words]\n",
    "\n",
    "        # remove punctuation from each token\n",
    "        #line = [w.translate(table) for w in line]\n",
    "\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "\n",
    "        # Format words and remove unwanted characters\n",
    "        text = \" \".join(line)\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "        # remove empty strings\n",
    "        if len(text )> 0 :\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of data using Lemmatization\n",
    "Lemmatization is used as it returns better words choice than stemming as Lemmatization returns only valid dictionary(wordnet) words. Trade is it takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    cleaned = list()\n",
    "\n",
    "    for line in text :\n",
    "        word_pos = nltk.pos_tag(nltk.word_tokenize(line))\n",
    "        lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "\n",
    "        word = [x.lower() for x in lemm_words]\n",
    "        cleaned.append(' '.join(word))\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all stories in a directory\n",
    "This is used to load and clean the learn and test dataset. After cleaning data it returns two list cleaned articles and cleaned headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stories(location):\n",
    "    stories = list()\n",
    "    file_list = listdir(location)\n",
    "    total_files = len (file_list)\n",
    "    count = 0\n",
    "    print (\"Total Files : {total_files}\".format(total_files= total_files))\n",
    "    clean_articles = []\n",
    "    clean_headlines = []\n",
    "    for name in file_list:\n",
    "        count = count + 1\n",
    "        filename = location + '/' + name\n",
    "        # load document\n",
    "        print('Loading  - {filename}, files number  - {count},  out of - {total_files}'\n",
    "              .format(filename=filename, count=count, total_files=total_files))\n",
    "        doc = load_files(filename)\n",
    "        # split into story and highlights\n",
    "        article, headlines = split_data(doc)\n",
    "        # store\n",
    "        #stories.append({'article': article, 'headlines' : headlines})\n",
    "\n",
    "        article = clean_text(article.split('\\n'))\n",
    "        article = normalize_text(article)\n",
    "        clean_articles.append(' '.join(article))\n",
    "        headlines = clean_text(headlines, remove_stopwords=False)\n",
    "        headlines = normalize_text(headlines)\n",
    "        clean_headlines.append(' '.join(headlines))\n",
    "\n",
    "    return clean_articles, clean_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program\n",
    "Start point of data cleaning, once the articles and headlines are cleaned, they dumped so that can be reused for vectorization and then running model directly. This is becasue cleaning is an expensive operation in terms of time and resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start = time.perf_counter()\n",
    "    clean_articles, clean_headlines = load_stories(path)\n",
    "    print(\"Total Articles  : {len_articles} , Total Headlines : {len_headlines}- Time Taken : {time_taken}\"\n",
    "          .format(len_articles=len(clean_articles), len_headlines =\n",
    "                  len(clean_headlines), time_taken = (time.perf_counter()-start)/60))\n",
    "\n",
    "    print (\"Serialization of articles\")\n",
    "    # Store Articles (serialize)\n",
    "    with open(base_path + articles_pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(clean_articles, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Serialization of headlines\")\n",
    "    # Store Articles (serialize)\n",
    "    with open(base_path + headlines_pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(clean_headlines, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "'''-------------------------main------------------------------'''\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained English word embedding\n",
    "\n",
    "This is used to load pre-trained english word embedding 'fast text' provided facebook. First it checks if pre-trained model dump already exists, if not it load model and put in it dump. Dump is created becasue it loads faster than actual word embedding model.\n",
    "https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_model():\n",
    "    model_pickle = Path(config.base_path + config.model_pickle_filename)\n",
    "    if model_pickle.exists():\n",
    "        print(\"Loading Pre-Trained Model Pickle..... \")\n",
    "        start = time.perf_counter()\n",
    "        with open(config.base_path + config.model_pickle_filename, 'rb') as handle:\n",
    "            model = pickle.load(handle)\n",
    "        print(\"Loaded Pre-Trained Model Pickle, time taken\", ((time.perf_counter() - start) / 60))\n",
    "    else:\n",
    "        print(\"Loading Pre-Trained Model  ..... \")\n",
    "        start = time.perf_counter()\n",
    "        model = KeyedVectors.load_word2vec_format(config.model_path, binary=False)\n",
    "        with open(config.base_path + config.model_pickle_filename, 'wb') as handle:\n",
    "            pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Loaded Pre-Trained Model, time taken\", ((time.perf_counter() - start) / 60))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count_words\n",
    "\n",
    "This is a utility method used to count how many times a word is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    ''' Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorization\n",
    "\n",
    "This is used to get word embedding for each word from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(text, embeddings_index, model):\n",
    "    for sentence in text:\n",
    "        try:\n",
    "            for vocab_word in sentence.split():\n",
    "                embeddings_index[vocab_word] = model[vocab_word]              \n",
    "                # print(\"Work : {vocab_word} , vector value : {vector_value}\".\n",
    "                #format(vocab_word=vocab_word, vector_value =vector_value))\n",
    "        except KeyError:\n",
    "            '''ignore'''\n",
    "            # print(\"{vocab_word} not in vocabulary\".format(vocab_word=vocab_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing_word_ratio\n",
    "\n",
    "Find the number of words that are missing from CN, and are used more than our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_word_ratio(word_counts, embeddings_index):\n",
    "    ''' Find the number of words that are missing from CN, and are used more than our threshold.'''\n",
    "    missing_words_count = 0\n",
    "    missing_words = list()\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        if word not in embeddings_index and word not in missing_words and count > threshold:\n",
    "            missing_words_count += 1\n",
    "            missing_words.append(word)\n",
    "            # print(\"{word} is missing \".format(word=word))\n",
    "\n",
    "    missing_ratio = round(missing_words_count / len(word_counts), 4) * 100\n",
    "    return missing_ratio, missing_words_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covert_vocab_to_int\n",
    "\n",
    "This is used to covert each word in training set to word vector. This is important as ML algorithm can only understand numbers. This integer representation of word is later passed encoder for word processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_vocab_to_int(word_counts, embeddings_index):\n",
    "    # dictionary to convert words to integers\n",
    "    vocab_to_int = {}\n",
    "\n",
    "    value = 0\n",
    "    for word, count in word_counts.items():\n",
    "        if count > threshold or word in embeddings_index:\n",
    "            vocab_to_int[word] = value\n",
    "            value += 1\n",
    "\n",
    "    # Special tokens that will be added to our vocab\n",
    "    codes = [\"<UNK>\", \"<PAD>\", \"<EOS>\", \"<GO>\"]\n",
    "\n",
    "    # Add codes to vocab\n",
    "    for code in codes:\n",
    "        vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "    # Dictionary to convert integers to words\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "\n",
    "    usage_ratio = round(len(vocab_to_int) / len(word_counts), 4) * 100\n",
    "\n",
    "    print(\"Total number of unique words:\", len(word_counts))\n",
    "    print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "    print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
    "\n",
    "    return vocab_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_combine_word_matrix\n",
    "\n",
    "Need to use 300 for embedding dimensions to match corpus(input data) vectors.\n",
    "This will return cobine matriz that would have 'embeddings_index' for from pre-trained word embedding plus \n",
    "random embedding generated for words missing in pre-trained word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combine_word_matrix(vocab_to_int, embeddings_index):\n",
    "    \n",
    "    nb_words = len(vocab_to_int)\n",
    "    # Create matrix with default values of zero\n",
    "    word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    for word, i in vocab_to_int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_embedding_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            # If word not in CN, create a random embedding for it\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            embeddings_index[word] = new_embedding\n",
    "            word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "    # Check if value matches len(vocab_to_int)\n",
    "    print(\"word_embedding_matrix length : \", len(word_embedding_matrix))\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding unknown words\n",
    "\n",
    "Convert words in text to an integer. If word is not in vocab_to_int, use UNK's integer.\n",
    "Total the number of words and UNKs. Add EOS token to the end of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, vocab_to_int, eos=False):    \n",
    "    ints = []\n",
    "    word_count = 0\n",
    "    unk_count = 0\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                # print(\"UNK Word : \", word)\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "\n",
    "    unk_percent = round(unk_count / word_count, 4) * 100\n",
    "\n",
    "    print(\"Total number of words : \", word_count)\n",
    "    print(\"Total number of UNKs : \", unk_count)\n",
    "    print(\"Percent of words that are UNK: {}%\".format(unk_percent))\n",
    "\n",
    "    return ints, word_count, unk_count\n",
    "\n",
    "\n",
    "def create_dataFrame(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])\n",
    "\n",
    "\n",
    "def unk_counter(sentence, vocab_to_int):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting training dataset\n",
    "\n",
    "Sort the summaries and texts by the length of the texts, shortest to longest. \n",
    "This is required so that batch provided to tensorflow will have lesser padding as sentences would be of same size.\n",
    "Limit the length of summaries and texts based on the min and max ranges. This is to avoid out of range data.\n",
    "Remove reviews that include too many UNKs as they would not provide much of learning experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_corplus(lengths_articles, int_rep_articles, int_rep_headlines, vocab_to_int):\n",
    "\n",
    "    sorted_articles = []\n",
    "    sorted_headlines = []\n",
    "    max_text_length = config.max_text_length\n",
    "    max_summary_length = config.max_summary_length\n",
    "    min_length = config.min_length\n",
    "    unk_text_limit = config.unk_text_limit\n",
    "    unk_summary_limit = 0\n",
    "\n",
    "    for count, words in enumerate(int_rep_articles):\n",
    "        if (len(int_rep_articles[count]) >= min_length and len(int_rep_articles[count]) <= max_text_length\n",
    "            and unk_counter(int_rep_headlines[count], vocab_to_int) <= unk_summary_limit and\n",
    "                    unk_counter(int_rep_articles[count], vocab_to_int) <= unk_text_limit):\n",
    "            sorted_headlines.append(int_rep_headlines[count])\n",
    "            sorted_articles.append(int_rep_articles[count])\n",
    "\n",
    "    # Compare lengths to ensure they match\n",
    "    print(len(sorted_headlines))\n",
    "    print(len(sorted_articles))\n",
    "\n",
    "    return sorted_articles, sorted_headlines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input for Tensorflow graph\n",
    "\n",
    "For using tensorflow we need to provide below input paramters and create_input_for_graph() is used to generate these variables.\n",
    "\n",
    "clean_articles -> articles after removing impurities\n",
    "sorted_articles -> articles sorted as the thr length\n",
    "sorted_headline -> headlines (sorted as per article length) as the thr length\n",
    "vocab_to_int -> interger values of all vocab words\n",
    "word_embedding_matrix -> 300 dim matrix for each word in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_for_graph():\n",
    "    # Load data (deserialize)\n",
    "    with open(config.base_path + config.articles_pickle_filename, 'rb') as handle:\n",
    "        clean_articles = pickle.load(handle)\n",
    "\n",
    "    with open(config.base_path + config.headlines_pickle_filename, 'rb') as handle:\n",
    "        clean_headlines = pickle.load(handle)\n",
    "\n",
    "    pre_trained_model = create_or_load_model()\n",
    "\n",
    "    word_counts = {}\n",
    "    print(\"counting  Articles\")\n",
    "    count_words(word_counts, clean_articles)\n",
    "    print(\"counting  Headlines\")\n",
    "    count_words(word_counts, clean_headlines)\n",
    "\n",
    "    print(\"Total Stories : \", len(clean_headlines))\n",
    "    print(\"Size of Vocabulary:\", len(word_counts))\n",
    "\n",
    "    print(\"creating embedding index .....\")\n",
    "    embeddings_index = {};\n",
    "    vectorization(clean_articles, embeddings_index, pre_trained_model)\n",
    "    vectorization(clean_headlines, embeddings_index, pre_trained_model)\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "\n",
    "    # find out missing words and thr %\n",
    "    missing_ratio, missing_words_count = missing_word_ratio(word_counts, embeddings_index)\n",
    "\n",
    "    print(\"Number of words missing :\", missing_words_count)\n",
    "    print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n",
    "\n",
    "    '''dictionary to convert words to integers - This is to found total words count that we get from aur corpus(input date)\n",
    "    and out of that what % of words we would be using. This is after removing words that count less than threshold'''\n",
    "    vocab_to_int = covert_vocab_to_int(word_counts, embeddings_index)\n",
    "\n",
    "    word_embedding_matrix = create_combine_word_matrix(vocab_to_int, embeddings_index)\n",
    "\n",
    "    # Apply convert_to_ints to clean_articles and clean_headlines\n",
    "    print(\"Article Data\")\n",
    "    int_repr_articles, word_article_count, unk_article_count = convert_to_ints(clean_articles, vocab_to_int, eos=True)\n",
    "\n",
    "    print(\"Headline Data\")\n",
    "    int_repr_headlines, word_headline_count, unk_headline_count = convert_to_ints(clean_headlines, vocab_to_int)\n",
    "\n",
    "    lengths_articles = create_dataFrame(int_repr_articles)\n",
    "    # lengths_headlines = create_dataFrame(int_repr_headlines)\n",
    "\n",
    "    sorted_articles, sorted_headlines = sort_corplus(lengths_articles, int_repr_articles,\n",
    "                                                     int_repr_headlines, vocab_to_int)\n",
    "\n",
    "    return vocab_to_int, word_embedding_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
