{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import listdir\n",
    "import time\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config & Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'G:\\\\AI\\\\data\\\\cnn\\\\'\n",
    "path = base_path + 'sample_5k\\\\'\n",
    "articles_pickle_filename = \"articles.pickle\"\n",
    "headlines_pickle_filename = \"headlines.pickle\"\n",
    "model_pickle_filename = \"model.pickle\"\n",
    "word_embedding_matrix_filename = \"word_embedding_matrix.pickle\"\n",
    "\n",
    "''' https://fasttext.cc/docs/en/english-vectors.html '''\n",
    "model_path ='G:\\Python\\MLLearning\\MachineLearning\\data\\wiki-news-300d-1M.vec'\n",
    "\n",
    "# to avoid words that are used less that threshold value\n",
    "threshold = 2\n",
    "\n",
    "# Dimension size as per pre-trained data\n",
    "embedding_dim = 300\n",
    "max_text_length = 1000\n",
    "max_summary_length = 20\n",
    "min_length = 2\n",
    "unk_text_limit = 200\n",
    "\n",
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword list and Initialize Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files and load into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split a document into news article body and headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(doc):\n",
    "    # find first headlines\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and headlines\n",
    "    article, headlines = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    headlines = [h.strip() for h in headlines if len(h) > 0]\n",
    "    return article, headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean a list of lines\n",
    "This section is used to remove unwanted words and return cleaned articles and headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lines, remove_stopwords=True):\n",
    "    \n",
    "    cleaned = list()\n",
    "    for line in lines:\n",
    "        # strip source cnn office if it exists\n",
    "        index = line.find('(CNN)  -- ')\n",
    "        if index > -1:\n",
    "            line = line[index + len('(CNN)'):]\n",
    "        else:\n",
    "            index = line.find('(CNN)')\n",
    "            if index > -1:\n",
    "                line = line[index + len('(CNN)'):]\n",
    "\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "\n",
    "        # Optionally, remove stop words\n",
    "        if remove_stopwords:\n",
    "            line = [w for w in line if w not in stop_words]\n",
    "\n",
    "        # remove punctuation from each token\n",
    "        #line = [w.translate(table) for w in line]\n",
    "\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "\n",
    "        # Format words and remove unwanted characters\n",
    "        text = \" \".join(line)\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\<a href', ' ', text)\n",
    "        text = re.sub(r'&amp;', '', text)\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "        # remove empty strings\n",
    "        if len(text )> 0 :\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of data using Lemmatization\n",
    "Lemmatization is used as it returns better words choice than stemming as Lemmatization returns only valid dictionary words. Trade is it takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    cleaned = list()\n",
    "\n",
    "    for line in text :\n",
    "        word_pos = nltk.pos_tag(nltk.word_tokenize(line))\n",
    "        lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "\n",
    "        word = [x.lower() for x in lemm_words]\n",
    "        cleaned.append(' '.join(word))\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all stories in a directory\n",
    "This is used to load and clean the learn and test dataset. After cleaning data it returns two list cleaned articles and cleaned headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stories(location):\n",
    "    stories = list()\n",
    "    file_list = listdir(location)\n",
    "    total_files = len (file_list)\n",
    "    count = 0\n",
    "    print (\"Total Files : {total_files}\".format(total_files= total_files))\n",
    "    clean_articles = []\n",
    "    clean_headlines = []\n",
    "    for name in file_list:\n",
    "        count = count + 1\n",
    "        filename = location + '/' + name\n",
    "        # load document\n",
    "        print('Loading  - {filename}, files number  - {count},  out of - {total_files}'\n",
    "              .format(filename=filename, count=count, total_files=total_files))\n",
    "        doc = load_files(filename)\n",
    "        # split into story and highlights\n",
    "        article, headlines = split_data(doc)\n",
    "        # store\n",
    "        #stories.append({'article': article, 'headlines' : headlines})\n",
    "\n",
    "        article = clean_text(article.split('\\n'))\n",
    "        article = normalize_text(article)\n",
    "        clean_articles.append(' '.join(article))\n",
    "        headlines = clean_text(headlines, remove_stopwords=False)\n",
    "        headlines = normalize_text(headlines)\n",
    "        clean_headlines.append(' '.join(headlines))\n",
    "\n",
    "    return clean_articles, clean_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program\n",
    "Start point of data cleaning, once the articles and headlines are cleaned, they dumped so that can be reused for vectorization and then running model directly. This is becasue cleaning is an expensive operation in terms of time and resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start = time.perf_counter()\n",
    "    clean_articles, clean_headlines = load_stories(path)\n",
    "    print(\"Total Articles  : {len_articles} , Total Headlines : {len_headlines}- Time Taken : {time_taken}\"\n",
    "          .format(len_articles=len(clean_articles), len_headlines =len(clean_headlines), time_taken = (time.perf_counter()-start)/60))\n",
    "\n",
    "    print (\"Serialization of articles\")\n",
    "    # Store Articles (serialize)\n",
    "    with open(base_path + articles_pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(clean_articles, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Serialization of headlines\")\n",
    "    # Store Articles (serialize)\n",
    "    with open(base_path + headlines_pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(clean_headlines, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "'''-------------------------main------------------------------'''\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
